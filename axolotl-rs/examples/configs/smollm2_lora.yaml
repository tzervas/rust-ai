# SmolLM2-135M LoRA Configuration (No Quantization)
# For testing LoRA without quantization overhead
#
# Usage:
#   cargo run --features peft -- train --config examples/configs/smollm2_lora.yaml

base_model: "HuggingFaceTB/SmolLM2-135M"
adapter: lora
output_dir: "./outputs/smollm2-lora"

lora:
  r: 16
  alpha: 32
  dropout: 0.05
  target_modules:
    - q_proj
    - k_proj
    - v_proj
    - o_proj

# No quantization section - pure LoRA

dataset:
  path: "./data/alpaca_1k.jsonl"
  type: alpaca
  max_length: 512
  train_split: 0.9

training:
  epochs: 2
  batch_size: 4
  learning_rate: 0.0001
  weight_decay: 0.0
  logging_steps: 20
  save_steps: 100
  warmup_ratio: 0.1

seed: 42
