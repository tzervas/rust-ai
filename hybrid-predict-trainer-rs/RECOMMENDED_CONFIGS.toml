# Recommended HybridTrainer Configurations
# Based on theoretical research from 2026-02-06
# See RESEARCH_OPTIMIZATION_SUMMARY.md for full analysis

# ═══════════════════════════════════════════════════════════════════
# CONSERVATIVE (Quality-First)
# ═══════════════════════════════════════════════════════════════════
# Use case: Production model training, fine-tuning where final quality is paramount
# Expected: 2.5-3.0x speedup, <0.5% quality loss, <5% divergence rate

[conservative]
warmup_steps = 200
full_steps = 30
max_predict_steps = 25
confidence_threshold = 0.90

[conservative.predictor_config]
type = "RSSM"
ensemble_size = 5              # Improved from default 3
deterministic_dim = 256
stochastic_dim = 32
num_categoricals = 32
learning_rate = 0.001
max_grad_norm = 5.0

[conservative.divergence_config]
loss_sigma_threshold = 2.2     # Tight but stable
gradient_norm_multiplier = 50.0
gradient_vanish_threshold = 0.01
check_interval_steps = 5
max_loss_relative_change = 0.3
max_oscillation_frequency = 0.3


# ═══════════════════════════════════════════════════════════════════
# BALANCED (Recommended Default)
# ═══════════════════════════════════════════════════════════════════
# Use case: General training where both speed and quality matter
# Expected: 3.5-4.5x speedup, <1.5% quality loss, <10% divergence rate
# ⭐ PARETO OPTIMAL - best speedup/quality tradeoff

[balanced]
warmup_steps = 100
full_steps = 20
max_predict_steps = 40         # Can push to 50 with Phase 1 improvements
confidence_threshold = 0.85

[balanced.predictor_config]
type = "RSSM"
ensemble_size = 5              # Phase 1 improvement (+22% horizon)
deterministic_dim = 256
stochastic_dim = 32
num_categoricals = 32
learning_rate = 0.001
max_grad_norm = 5.0

[balanced.divergence_config]
loss_sigma_threshold = 2.5     # Sweet spot from research
gradient_norm_multiplier = 100.0
gradient_vanish_threshold = 0.01
check_interval_steps = 10
max_loss_relative_change = 0.5
max_oscillation_frequency = 0.5


# ═══════════════════════════════════════════════════════════════════
# AGGRESSIVE (Speed-First)
# ═══════════════════════════════════════════════════════════════════
# Use case: Experimentation, hyperparameter search, early training epochs
# Expected: 5-8x speedup, 2-5% quality loss, 15-25% divergence rate

[aggressive]
warmup_steps = 50
full_steps = 15
max_predict_steps = 80
confidence_threshold = 0.75

[aggressive.predictor_config]
type = "RSSM"
ensemble_size = 3              # Default is fine for speed-first
deterministic_dim = 256
stochastic_dim = 32
num_categoricals = 32
learning_rate = 0.001
max_grad_norm = 5.0

[aggressive.divergence_config]
loss_sigma_threshold = 3.5     # Very loose
gradient_norm_multiplier = 200.0
gradient_vanish_threshold = 0.01
check_interval_steps = 20
max_loss_relative_change = 1.0
max_oscillation_frequency = 0.7


# ═══════════════════════════════════════════════════════════════════
# FUTURE: With Phase 2 Improvements
# ═══════════════════════════════════════════════════════════════════
# After implementing:
# - Intra-horizon micro-corrections (correction_interval)
# - Multi-step BPTT k=3-5
# Expected: 5-7x speedup, <1.5% quality loss, <8% divergence rate

[future_enhanced]
warmup_steps = 100
full_steps = 20
max_predict_steps = 80         # Can extend to 120+ with micro-corrections
confidence_threshold = 0.85
correction_interval = 12       # Apply micro-corrections every 12 predicted steps

[future_enhanced.predictor_config]
type = "RSSM"
ensemble_size = 5
deterministic_dim = 256
stochastic_dim = 32
num_categoricals = 32
learning_rate = 0.001
max_grad_norm = 5.0
bptt_steps = 3                 # Multi-step BPTT (not yet implemented)

[future_enhanced.divergence_config]
loss_sigma_threshold = 2.5
adaptive_sigma = true          # Auto-adjust based on loss CV (not yet implemented)
gradient_norm_multiplier = 100.0
gradient_vanish_threshold = 0.01
check_interval_steps = 10
max_loss_relative_change = 0.5
max_oscillation_frequency = 0.5


# ═══════════════════════════════════════════════════════════════════
# NOTES
# ═══════════════════════════════════════════════════════════════════

# Sigma Threshold Guide:
# - 2.0: Very tight, quality-first, 22-step max horizon
# - 2.2: Tight but balanced, 25-step max horizon (conservative)
# - 2.5: Sweet spot, 30-step max horizon (recommended)
# - 3.0: Loose, speed-first, 35-step max horizon
# - 3.5: Very loose, experimental, 42-step max horizon

# Error Accumulation Formula:
# E(H) = e₁ × H^1.5 × (1 + 0.05 × max_curvature)
# where e₁ ≈ 0.03 × loss_ema for a trained predictor

# Max Horizon Derivation:
# H_max = (σ × loss_std / e₁)^(2/3)
# Example: σ=2.5, loss_std/loss_ema=0.05, e₁=0.03×L
#   → H_max = (2.5 × 0.05 / 0.03)^(2/3) = 4.17^(2/3) ≈ 2.7 → ~30 steps

# Ensemble Size Impact:
# Calibration error ∝ 1/√K
# K=3 (baseline): 1.00x error
# K=5: 0.77x error (23% better)
# K=7: 0.65x error (35% better)
# Memory: ~1MB per ensemble member (~5MB for K=5)

# Phase 1 Improvements (IMPLEMENTED):
# 1. Ensemble size 3→5: +22% longer horizons
# 2. Adaptive correction scaling: 0.8-1.3x improvement
# 3. Evolving feature vector: 40-60% longer horizons (CRITICAL)

# Phase 2 Improvements (NOT YET IMPLEMENTED):
# 1. Intra-horizon micro-corrections: 8.4x error reduction
# 2. Multi-step BPTT k=3: 44% longer horizons
# 3. Adaptive sigma: Auto-tuning based on training phase
