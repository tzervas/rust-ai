#!/usr/bin/env python3
"""
Dataset Enrichment Pipeline for Tritter Model Training

Adds quality signals, metadata tags, and filters to create
the best possible training dataset.

Enrichment stages:
1. Quality scoring (complexity, readability, documentation)
2. Metadata tagging (language, domain, difficulty)
3. Deduplication (exact and near-duplicate)
4. Filtering (remove low quality, boilerplate)
5. Instruction pair generation (optional)
"""

import json
import re
import hashlib
import sys
from pathlib import Path
from typing import Dict, List, Optional, Tuple, Any
from dataclasses import dataclass, field, asdict
from collections import defaultdict
import argparse

# Try to import optional dependencies
try:
    from datasketch import MinHash, MinHashLSH
    HAS_DATASKETCH = True
except ImportError:
    HAS_DATASKETCH = False
    print("Warning: datasketch not installed, near-duplicate detection disabled")

# Configuration
INPUT_DIR = Path("/data/datasets/tritter")
OUTPUT_DIR = Path("/data/datasets/tritter/enriched")

# Quality thresholds
MIN_LINE_COUNT = 5
MAX_LINE_COUNT = 10000
MIN_CHAR_COUNT = 100
MAX_CHAR_COUNT = 500000
MAX_LINE_LENGTH = 500
MIN_ALPHA_RATIO = 0.3  # Minimum ratio of alphabetic characters


@dataclass
class QualityMetrics:
    """Quality metrics for a code/text sample."""
    line_count: int = 0
    char_count: int = 0
    avg_line_length: float = 0.0
    max_line_length: int = 0
    alpha_ratio: float = 0.0
    comment_ratio: float = 0.0
    has_docstring: bool = False
    has_imports: bool = False
    complexity_score: float = 0.0  # 0-1, higher = more complex
    quality_score: float = 0.0     # 0-1, higher = better quality


@dataclass
class EnrichedRecord:
    """Enriched training record with metadata and quality signals."""
    text: str
    meta: Dict[str, Any] = field(default_factory=dict)
    quality: Dict[str, Any] = field(default_factory=dict)
    tags: List[str] = field(default_factory=list)
    instruction_pair: Optional[Dict[str, str]] = None


# Language detection patterns
LANGUAGE_PATTERNS = {
    'python': [r'^\s*def\s+\w+\s*\(', r'^\s*class\s+\w+', r'^\s*import\s+', r'^\s*from\s+\w+\s+import'],
    'rust': [r'^\s*fn\s+\w+', r'^\s*struct\s+\w+', r'^\s*impl\s+', r'^\s*use\s+\w+::'],
    'typescript': [r'^\s*interface\s+\w+', r'^\s*type\s+\w+\s*=', r':\s*\w+\s*[=;]', r'^\s*export\s+'],
    'javascript': [r'^\s*function\s+\w+', r'^\s*const\s+\w+\s*=', r'^\s*let\s+\w+\s*=', r'=>'],
    'go': [r'^\s*func\s+', r'^\s*type\s+\w+\s+struct', r'^\s*package\s+\w+'],
    'java': [r'^\s*public\s+class', r'^\s*private\s+', r'^\s*@\w+'],
    'terraform': [r'^\s*resource\s+"', r'^\s*provider\s+"', r'^\s*variable\s+"', r'^\s*module\s+"'],
    'yaml': [r'^\w+:\s*$', r'^\s*-\s+\w+:', r'^apiVersion:', r'^kind:'],
    'dockerfile': [r'^FROM\s+', r'^RUN\s+', r'^CMD\s+', r'^COPY\s+'],
}

# Domain patterns
DOMAIN_PATTERNS = {
    'web': ['flask', 'django', 'fastapi', 'express', 'react', 'vue', 'angular', 'nextjs'],
    'ml': ['torch', 'tensorflow', 'keras', 'sklearn', 'numpy', 'pandas', 'transformers', 'huggingface'],
    'systems': ['socket', 'thread', 'process', 'mutex', 'async', 'tokio', 'crossbeam'],
    'data': ['sql', 'database', 'postgres', 'mysql', 'mongodb', 'redis', 'elasticsearch'],
    'devops': ['docker', 'kubernetes', 'terraform', 'ansible', 'github', 'ci', 'cd', 'pipeline'],
    'security': ['auth', 'encrypt', 'hash', 'token', 'jwt', 'oauth', 'ssl', 'tls'],
    'testing': ['test', 'assert', 'mock', 'pytest', 'jest', 'unittest', 'spec'],
}

# Boilerplate patterns to filter
BOILERPLATE_PATTERNS = [
    r'^#\s*auto-?generated',
    r'^//\s*auto-?generated',
    r'DO NOT EDIT',
    r'AUTO-GENERATED',
    r'This file was generated',
    r'generated by \w+',
]


def calculate_quality_metrics(text: str, language: str = None) -> QualityMetrics:
    """Calculate quality metrics for a text sample."""
    lines = text.split('\n')
    metrics = QualityMetrics()

    metrics.line_count = len(lines)
    metrics.char_count = len(text)

    if not lines:
        return metrics

    line_lengths = [len(line) for line in lines]
    metrics.avg_line_length = sum(line_lengths) / len(lines)
    metrics.max_line_length = max(line_lengths) if line_lengths else 0

    # Alpha ratio
    alpha_count = sum(1 for c in text if c.isalpha())
    metrics.alpha_ratio = alpha_count / len(text) if text else 0

    # Comment ratio (approximate)
    comment_lines = sum(1 for line in lines if line.strip().startswith(('#', '//', '/*', '*', '--')))
    metrics.comment_ratio = comment_lines / len(lines) if lines else 0

    # Check for docstrings/documentation
    metrics.has_docstring = '"""' in text or "'''" in text or '///' in text or '/**' in text

    # Check for imports
    metrics.has_imports = bool(re.search(r'^(import|from|use|require|include)\s+', text, re.MULTILINE))

    # Complexity score (based on nesting, branching)
    nesting_indicators = len(re.findall(r'[\{\[\(]', text))
    branch_indicators = len(re.findall(r'\b(if|else|elif|match|case|for|while|loop)\b', text))
    func_indicators = len(re.findall(r'\b(fn|def|func|function)\b', text))

    complexity_raw = (nesting_indicators * 0.1 + branch_indicators * 0.5 + func_indicators * 0.3)
    metrics.complexity_score = min(1.0, complexity_raw / 50)  # Normalize

    # Overall quality score
    quality_factors = []

    # Line count factor (prefer medium-sized files)
    if 10 <= metrics.line_count <= 500:
        quality_factors.append(1.0)
    elif metrics.line_count < 10:
        quality_factors.append(0.3)
    else:
        quality_factors.append(0.7)

    # Alpha ratio factor
    quality_factors.append(min(1.0, metrics.alpha_ratio * 2))

    # Comment ratio factor (some comments are good)
    if 0.05 <= metrics.comment_ratio <= 0.3:
        quality_factors.append(1.0)
    elif metrics.comment_ratio > 0.5:
        quality_factors.append(0.3)  # Too many comments
    else:
        quality_factors.append(0.7)

    # Documentation factor
    if metrics.has_docstring:
        quality_factors.append(1.0)
    else:
        quality_factors.append(0.6)

    # Line length factor
    if metrics.avg_line_length < 100:
        quality_factors.append(1.0)
    else:
        quality_factors.append(0.6)

    metrics.quality_score = sum(quality_factors) / len(quality_factors)

    return metrics


def detect_language(text: str, filename: str = None) -> str:
    """Detect programming language from content and filename."""
    if filename:
        ext = Path(filename).suffix.lower()
        ext_map = {
            '.py': 'python',
            '.rs': 'rust',
            '.ts': 'typescript',
            '.tsx': 'typescript',
            '.js': 'javascript',
            '.jsx': 'javascript',
            '.go': 'go',
            '.java': 'java',
            '.tf': 'terraform',
            '.tfvars': 'terraform',
            '.yaml': 'yaml',
            '.yml': 'yaml',
        }
        if ext in ext_map:
            return ext_map[ext]

        if 'Dockerfile' in filename:
            return 'dockerfile'

    # Content-based detection
    for lang, patterns in LANGUAGE_PATTERNS.items():
        matches = sum(1 for p in patterns if re.search(p, text, re.MULTILINE))
        if matches >= 2:
            return lang

    return 'unknown'


def detect_domains(text: str) -> List[str]:
    """Detect domains based on content patterns."""
    text_lower = text.lower()
    detected = []

    for domain, keywords in DOMAIN_PATTERNS.items():
        if any(kw in text_lower for kw in keywords):
            detected.append(domain)

    return detected if detected else ['general']


def is_boilerplate(text: str) -> bool:
    """Check if text is auto-generated boilerplate."""
    first_lines = '\n'.join(text.split('\n')[:10])
    for pattern in BOILERPLATE_PATTERNS:
        if re.search(pattern, first_lines, re.IGNORECASE):
            return True
    return False


def calculate_minhash(text: str, num_perm: int = 128) -> Optional['MinHash']:
    """Calculate MinHash for near-duplicate detection."""
    if not HAS_DATASKETCH:
        return None

    m = MinHash(num_perm=num_perm)

    # Use 5-grams of words
    words = text.lower().split()
    for i in range(len(words) - 4):
        shingle = ' '.join(words[i:i+5])
        m.update(shingle.encode('utf-8'))

    return m


def should_filter(text: str, metrics: QualityMetrics) -> Tuple[bool, str]:
    """Determine if a sample should be filtered out."""
    # Size filters
    if metrics.line_count < MIN_LINE_COUNT:
        return True, "too_few_lines"
    if metrics.line_count > MAX_LINE_COUNT:
        return True, "too_many_lines"
    if metrics.char_count < MIN_CHAR_COUNT:
        return True, "too_short"
    if metrics.char_count > MAX_CHAR_COUNT:
        return True, "too_long"

    # Quality filters
    if metrics.alpha_ratio < MIN_ALPHA_RATIO:
        return True, "low_alpha_ratio"
    if metrics.max_line_length > MAX_LINE_LENGTH:
        return True, "line_too_long"

    # Content filters
    if is_boilerplate(text):
        return True, "boilerplate"

    # Quality threshold
    if metrics.quality_score < 0.4:
        return True, "low_quality"

    return False, ""


def generate_instruction_pair(text: str, language: str) -> Optional[Dict[str, str]]:
    """Generate instruction-response pair from code."""
    # Try to extract docstring and code
    if language == 'python':
        # Look for function with docstring
        match = re.search(
            r'def\s+(\w+)\s*\([^)]*\):\s*(?:"""([^"]+)"""|\'\'\'([^\']+)\'\'\')\s*(.+?)(?=\ndef|\Z)',
            text,
            re.DOTALL
        )
        if match:
            func_name = match.group(1)
            docstring = match.group(2) or match.group(3)
            body = match.group(4).strip()
            if docstring and body:
                return {
                    "instruction": f"Write a Python function called `{func_name}` that {docstring.strip().lower()}",
                    "response": f"def {func_name}{text[match.start():match.end()]}"
                }

    elif language == 'rust':
        # Look for function with doc comments
        match = re.search(
            r'///\s*(.+?)\n(?:///.*\n)*pub\s+fn\s+(\w+)',
            text,
            re.MULTILINE
        )
        if match:
            doc = match.group(1).strip()
            func_name = match.group(2)
            return {
                "instruction": f"Write a Rust function called `{func_name}` that {doc.lower()}",
                "response": text[match.start():].split('\n}\n')[0] + '\n}'
            }

    return None


def enrich_record(record: Dict, lsh: Optional['MinHashLSH'] = None) -> Optional[EnrichedRecord]:
    """Enrich a single record with quality metrics and metadata."""
    text = record.get('text', '')
    meta = record.get('meta', {})

    if not text.strip():
        return None

    # Calculate quality metrics
    language = detect_language(text, meta.get('path', ''))
    metrics = calculate_quality_metrics(text, language)

    # Filter check
    should_skip, reason = should_filter(text, metrics)
    if should_skip:
        return None

    # Near-duplicate check
    if lsh and HAS_DATASKETCH:
        minhash = calculate_minhash(text)
        if minhash:
            # Check for duplicates
            existing = lsh.query(minhash)
            if existing:
                return None  # Skip duplicate

            # Add to LSH
            doc_id = hashlib.sha256(text.encode()).hexdigest()[:16]
            lsh.insert(doc_id, minhash)

    # Detect domains
    domains = detect_domains(text)

    # Build enriched record
    enriched = EnrichedRecord(
        text=text,
        meta={
            **meta,
            'language': language,
            'domains': domains,
        },
        quality={
            'line_count': metrics.line_count,
            'char_count': metrics.char_count,
            'quality_score': round(metrics.quality_score, 3),
            'complexity_score': round(metrics.complexity_score, 3),
            'has_documentation': metrics.has_docstring,
        },
        tags=[language] + domains,
    )

    # Generate instruction pair if applicable
    if language in ['python', 'rust', 'typescript', 'go']:
        pair = generate_instruction_pair(text, language)
        if pair:
            enriched.instruction_pair = pair

    return enriched


def process_jsonl_file(input_path: Path, output_path: Path, lsh: Optional['MinHashLSH'] = None) -> Dict[str, int]:
    """Process a JSONL file and write enriched output."""
    stats = defaultdict(int)

    with open(input_path, 'r', encoding='utf-8') as infile, \
         open(output_path, 'w', encoding='utf-8') as outfile:

        for line in infile:
            stats['total'] += 1

            try:
                record = json.loads(line)
                enriched = enrich_record(record, lsh)

                if enriched:
                    output = {
                        'text': enriched.text,
                        'meta': enriched.meta,
                        'quality': enriched.quality,
                        'tags': enriched.tags,
                    }
                    if enriched.instruction_pair:
                        output['instruction_pair'] = enriched.instruction_pair

                    outfile.write(json.dumps(output, ensure_ascii=False) + '\n')
                    stats['kept'] += 1

                    # Track by language
                    lang = enriched.meta.get('language', 'unknown')
                    stats[f'lang_{lang}'] += 1
                else:
                    stats['filtered'] += 1

            except json.JSONDecodeError:
                stats['parse_error'] += 1
            except Exception as e:
                stats['error'] += 1

    return dict(stats)


def main():
    parser = argparse.ArgumentParser(description='Enrich dataset for Tritter training')
    parser.add_argument('--input-dir', type=Path, default=INPUT_DIR, help='Input directory')
    parser.add_argument('--output-dir', type=Path, default=OUTPUT_DIR, help='Output directory')
    parser.add_argument('--dedupe', action='store_true', help='Enable near-duplicate detection')
    args = parser.parse_args()

    input_dir = args.input_dir
    output_dir = args.output_dir
    output_dir.mkdir(parents=True, exist_ok=True)

    # Initialize LSH for deduplication if enabled
    lsh = None
    if args.dedupe and HAS_DATASKETCH:
        print("Initializing MinHash LSH for near-duplicate detection...")
        lsh = MinHashLSH(threshold=0.8, num_perm=128)

    # Find all JSONL files
    jsonl_files = list(input_dir.rglob('*.jsonl'))
    print(f"Found {len(jsonl_files)} JSONL files to process")

    all_stats = defaultdict(int)

    for jsonl_path in jsonl_files:
        # Create relative output path
        rel_path = jsonl_path.relative_to(input_dir)
        out_path = output_dir / rel_path.parent / f"enriched_{rel_path.name}"
        out_path.parent.mkdir(parents=True, exist_ok=True)

        print(f"Processing: {rel_path}")
        stats = process_jsonl_file(jsonl_path, out_path, lsh)

        for k, v in stats.items():
            all_stats[k] += v

        kept = stats.get('kept', 0)
        total = stats.get('total', 0)
        pct = (kept / total * 100) if total > 0 else 0
        print(f"  Kept {kept}/{total} ({pct:.1f}%)")

    print("\n=== Final Statistics ===")
    print(f"Total records: {all_stats['total']}")
    print(f"Kept: {all_stats['kept']} ({all_stats['kept']/all_stats['total']*100:.1f}%)")
    print(f"Filtered: {all_stats['filtered']}")
    print(f"Errors: {all_stats.get('error', 0) + all_stats.get('parse_error', 0)}")

    # Language breakdown
    print("\n=== Language Distribution ===")
    lang_stats = {k: v for k, v in all_stats.items() if k.startswith('lang_')}
    for lang, count in sorted(lang_stats.items(), key=lambda x: -x[1]):
        print(f"  {lang.replace('lang_', '')}: {count}")

    # Write metadata
    metadata = {
        'stats': dict(all_stats),
        'config': {
            'min_lines': MIN_LINE_COUNT,
            'max_lines': MAX_LINE_COUNT,
            'deduplication': args.dedupe,
        }
    }
    with open(output_dir / 'enrichment_metadata.json', 'w') as f:
        json.dump(metadata, f, indent=2)

    print(f"\nOutput directory: {output_dir}")


if __name__ == "__main__":
    main()
